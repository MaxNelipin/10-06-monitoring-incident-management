# 10-06-monitoring-incident-management

| Tables        | Are           | 
| ------------- |:-------------| 
| Краткое описание инцидента      | Отображению устаревшей и непоследовательной информации в течении 24 часа 11 минут, начиная с 2018 21 октября 22:52 UTC, в большинстве случаев GitHub также не мог обслуживать события веб-перехватчиков или создавать и публиковать сайты GitHub Pages.|
| Предшествующие события      | Регламентные работ по замене вышедшего из строя оптического оборудования 100G      |
| Причина инцидента | Была потеряна связь между нашим сетевым концентратором на восточном побережье США и нашим основным центром обработки данных на восточном побережье США  Согласно консенсусу Raft, во время описанного выше раздела сети Orchestrator, который был активен в нашем основном центре обработки данных, начал процесс отмены выбора руководства. Центр обработки данных на западном побережье США и узлы Orchestrator общедоступного облака на восточном побережье США смогли установить кворум и начать аварийное переключение кластеров для направления операций записи в центр обработки данных на западном побережье США. Orchestrator приступил к организации топологии кластера базы данных Западного побережья США. |
| Воздействие  | Серверы баз данных в центре обработки данных на восточном побережье США содержали короткий период записи, который не был реплицирован на объект на западном побережье США. Поскольку кластеры баз данных в обоих центрах обработки данных теперь содержали записи, которых не было в другом центре обработки данных, мы не смогли безопасно выполнить возврат основного сервера в центр обработки данных на восточном побережье США |
| Обнаружение | 2018 21 октября 22:54 UTC Наши внутренние системы мониторинга начали генерировать предупреждения, указывающие на многочисленные сбои в наших системах. В это время несколько инженеров отвечали и работали над сортировкой входящих уведомлений. К 23:02 UTC инженеры нашей группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в непредвиденном состоянии. Запрос API Orchestrator показал топологию репликации базы данных, которая включала только серверы из нашего центра обработки данных на западном побережье США.      |
| Реакция  | 2018 21 октября 23:19 UTC Из запроса состояния кластеров базы данных стало ясно, что нам нужно остановить выполнение заданий, записывающих метаданные о таких вещах, как push-уведомления. Мы сделали явный выбор частично ухудшить удобство использования сайта, приостановив доставку веб-перехватчика и сборки GitHub Pages вместо того, чтобы подвергать опасности данные, которые мы уже получили от пользователей. Другими словами, наша стратегия заключалась в том, чтобы отдавать предпочтение целостности данных, а не удобству использования сайта и времени восстановления.     |
| Восстановление  | Восстановить данные из резервных копий, синхронизировать реплики на обоих сайтах, вернуться к стабильной топологии обслуживания, а затем возобновить обработку заданий в очереди     |
| Таймлайн  |
||2018 21 октября 22:52 UTC Разрыв сетевой связности, Оркестратор перенапраляет операции записи на ДЦ Западного побережья 
||2018 21 октября 22:54 UTC Мониторинг начал генерировать предупреждения, указывающие на многочисленные сбои в наших системах 
||2018 21 октября 23:02 UTC инженеры  группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в непредвиденном состоянии
||2018 21 октября 23:07 UTC Вручную блокируется инструмент развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений.
||2018 21 октября 23:09 UTC Сайт переводится в жёлтый статус
||2018 21 октября 23:13 UTC Координатор переключает сайт красный статус, начинается работа по переводу операций записи на ДЦ Восточного побережья
||2018 21 октября 23:19 UTC Отключение записи метаданных событий и других вещей, влияющих на удобство сайта, чтобы сохранить данные пользователей 
||2018 22 октября 00:05 UTC Начата разработка плана восстановления
||2018 22 октября 00:41 UTC Начат процесс резервного копирования для всех затронутых кластеров БД
||2018 22 октября 06:51 UTC Несколько кластеров завершили восстановление из резервных копий в нашем центре обработки данных на восточном побережье США и начали репликацию новых данных с западного побережья
||2018 22 октября 07:46 UTC Публикация сообщения об инциденте в блоге
||2018 22 октября 11:12 UTC Все первичные базы данных снова установлены на восточном побережье США, но репликация начала замедляться из-за начала рабочего дня. Увеличено количество реплик чтения в ДЦ восточного побережья
||2018 22 октября 13:15 UTC Время репликация начало уменьшаться из-за доступности большего количества реплик чтения на востоыном побережье 
||2018 22 октября 16:24 UTC Реплики синхронизированы, аварийное переключение на исходную топологию
||2018 22 октября 16:45 UTC Начата доствка уведомлений и сборок страниц
||2018 22 октября 16:45 UTC Все уведомление и сборки доставлены, сайт переведён в зелёный статус
| Последующие действия |  Технические инициативы
||Настройте конфигурацию Orchestrator, чтобы предотвратить продвижение основных баз данных через региональные границы. Действия Оркестратора вели себя так, как настроено, несмотря на то, что наш уровень приложений не смог поддержать это изменение топологии. Выборы лидеров в регионе, как правило, безопасны, но внезапная задержка между странами стала основным фактором, способствовавшим этому инциденту. Это было неожиданное поведение системы, учитывая, что ранее мы не видели внутреннего сетевого раздела такого масштаба.
||Мы ускорили переход на новый механизм отчетов о состоянии, который предоставит нам более богатый форум для обсуждения активных инцидентов более четким и понятным языком.
||За несколько недель до этого инцидента мы начали общекорпоративную инженерную инициативу по поддержке обслуживания трафика GitHub из нескольких центров обработки данных в схеме «активный/активный/активный». Целью этого проекта является поддержка резервирования N+1 на уровне объекта. Цель этой работы — допустить полный отказ одного центра обработки данных без воздействия на пользователя. Это серьезное усилие, которое займет некоторое время, но мы считаем, что наличие нескольких сайтов с хорошей связью в одном регионе обеспечивает хороший набор компромиссов. Этот инцидент добавил актуальности инициативе.
||Мы займем более активную позицию в проверке наших предположений. GitHub — быстрорастущая компания, и за последнее десятилетие она значительно усложнилась. По мере того, как мы продолжаем расти, становится все труднее фиксировать и передавать исторический контекст компромиссов и решений, принятых новым поколениям Хабберов.
||Организационные инициативы
||Этот инцидент изменил наше отношение к надежности сайта. Мы узнали, что более строгий операционный контроль или сокращение времени отклика не являются достаточными гарантиями надежности сайта в рамках такой сложной системы услуг, как наша. Чтобы поддержать эти усилия, мы также начнем системную практику проверки сценариев сбоев, прежде чем они смогут повлиять на вас. Эта работа потребует будущих инвестиций в инструменты внедрения ошибок и хаос-инжиниринга в GitHub.
