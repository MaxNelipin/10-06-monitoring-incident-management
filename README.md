# 10-06-monitoring-incident-management

| Tables        | Are           | 
| ------------- |:-------------:| 
| Краткое описание инцидента      | Отображению устаревшей и непоследовательной информации в течении 24 часа 11 минут, начиная с 2018 21 октября 22:52 UTC, в большинстве случаев GitHub также не мог обслуживать события веб-перехватчиков или создавать и публиковать сайты GitHub Pages.|
| Предшествующие события      | Регламентные работ по замене вышедшего из строя оптического оборудования 100G      |
| Причина инцидента | Была потеряна связь между нашим сетевым концентратором на восточном побережье США и нашим основным центром обработки данных на восточном побережье США  Согласно консенсусу Raft, во время описанного выше раздела сети Orchestrator, который был активен в нашем основном центре обработки данных, начал процесс отмены выбора руководства. Центр обработки данных на западном побережье США и узлы Orchestrator общедоступного облака на восточном побережье США смогли установить кворум и начать аварийное переключение кластеров для направления операций записи в центр обработки данных на западном побережье США. Orchestrator приступил к организации топологии кластера базы данных Западного побережья США. |
| Воздействие  | Серверы баз данных в центре обработки данных на восточном побережье США содержали короткий период записи, который не был реплицирован на объект на западном побережье США. Поскольку кластеры баз данных в обоих центрах обработки данных теперь содержали записи, которых не было в другом центре обработки данных, мы не смогли безопасно выполнить возврат основного сервера в центр обработки данных на восточном побережье США |
| Обнаружение | 2018 21 октября 22:54 UTC Наши внутренние системы мониторинга начали генерировать предупреждения, указывающие на многочисленные сбои в наших системах. В это время несколько инженеров отвечали и работали над сортировкой входящих уведомлений. К 23:02 UTC инженеры нашей группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в непредвиденном состоянии. Запрос API Orchestrator показал топологию репликации базы данных, которая включала только серверы из нашего центра обработки данных на западном побережье США.      |
| Реакция  | 2018 21 октября 23:19 UTC Из запроса состояния кластеров базы данных стало ясно, что нам нужно остановить выполнение заданий, записывающих метаданные о таких вещах, как push-уведомления. Мы сделали явный выбор частично ухудшить удобство использования сайта, приостановив доставку веб-перехватчика и сборки GitHub Pages вместо того, чтобы подвергать опасности данные, которые мы уже получили от пользователей. Другими словами, наша стратегия заключалась в том, чтобы отдавать предпочтение целостности данных, а не удобству использования сайта и времени восстановления.     |
| Восстановление  | Восстановить данные из резервных копий, синхронизировать реплики на обоих сайтах, вернуться к стабильной топологии обслуживания, а затем возобновить обработку заданий в очереди     |
| Таймлайн  |      |
| Последующие действия |  Технические инициативы
||Настройте конфигурацию Orchestrator, чтобы предотвратить продвижение основных баз данных через региональные границы. Действия Оркестратора вели себя так, как настроено, несмотря на то, что наш уровень приложений не смог поддержать это изменение топологии. Выборы лидеров в регионе, как правило, безопасны, но внезапная задержка между странами стала основным фактором, способствовавшим этому инциденту. Это было неожиданное поведение системы, учитывая, что ранее мы не видели внутреннего сетевого раздела такого масштаба.
||Мы ускорили переход на новый механизм отчетов о состоянии, который предоставит нам более богатый форум для обсуждения активных инцидентов более четким и понятным языком.
||За несколько недель до этого инцидента мы начали общекорпоративную инженерную инициативу по поддержке обслуживания трафика GitHub из нескольких центров обработки данных в схеме «активный/активный/активный». Целью этого проекта является поддержка резервирования N+1 на уровне объекта. Цель этой работы — допустить полный отказ одного центра обработки данных без воздействия на пользователя. Это серьезное усилие, которое займет некоторое время, но мы считаем, что наличие нескольких сайтов с хорошей связью в одном регионе обеспечивает хороший набор компромиссов. Этот инцидент добавил актуальности инициативе.
||Мы займем более активную позицию в проверке наших предположений. GitHub — быстрорастущая компания, и за последнее десятилетие она значительно усложнилась. По мере того, как мы продолжаем расти, становится все труднее фиксировать и передавать исторический контекст компромиссов и решений, принятых новым поколениям Хабберов.
||Организационные инициативы
||Этот инцидент изменил наше отношение к надежности сайта. Мы узнали, что более строгий операционный контроль или сокращение времени отклика не являются достаточными гарантиями надежности сайта в рамках такой сложной системы услуг, как наша. Чтобы поддержать эти усилия, мы также начнем системную практику проверки сценариев сбоев, прежде чем они смогут повлиять на вас. Эта работа потребует будущих инвестиций в инструменты внедрения ошибок и хаос-инжиниринга в GitHub.

    |
